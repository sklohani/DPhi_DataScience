{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdPA03q3vRei"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JuiXnrL99eZzIiRzdtREyLlCYhF6Nc4u?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N4yt1psqgh1X"
   },
   "source": [
    "# Performance Evaluation, Cross Validation and Hyper - Parameter Tunning\n",
    "In this Notebook, we will learn 3 things:   \n",
    "*  Evaluation metrics\n",
    "*  Cross Validation\n",
    "*  Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "57GJ0KnnpC34"
   },
   "source": [
    "### Data Description\n",
    "Here we will use diabetes dataset for classification. Given different medical specifications about a person, we have to predict if the person have diabetes or not.\n",
    "\n",
    "**Different Attributes:**\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "9. Class variable (0 or 1)\n",
    "\n",
    "All the feature names are numerical. Let's give textual names to these features.\n",
    "*  Number of times pregnant: **num_preg**\n",
    "*  Plasma glucose concentration a 2 hours in an oral glucose tolerance test: **plasma_glucose_conc**\n",
    "*  Diastolic blood pressure (mm Hg): **D_blood_pressure**\n",
    "*  Triceps skin fold thickness (mm): **skin_fold_thickness**\n",
    "*  2-Hour serum insulin (mu U/ml): **serum_insulin**\n",
    "*  Body mass index (weight in kg/(height in m)^2): **body_mass_index**\n",
    "*  Diabetes pedigree function: **pedigree_func**\n",
    "*  Age (years): **age**\n",
    "*  Class variable (0 or 1): **diabetes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYnubTr4oUKY"
   },
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7WQljMFOgh1a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRSnYQG9okA4"
   },
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUaCOsuroooy"
   },
   "outputs": [],
   "source": [
    "# since the column names are numerical, we will give our own column names for our understanding\n",
    "col = [\"num_preg\", \"plasma_glucose_conc\", \"D_blood_pressure\", \"skin_fold_thickness\", \"serum_insulin\", \"body_mass_index\", \"pedigree_func\", \"age\", \"diabetes\"]\n",
    "diabetes_data = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/diabetes.txt\", names = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "e3Gg7tKkv8uw",
    "outputId": "ebbf6e56-c44d-4d57-becd-5ce895ea6171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_preg</th>\n",
       "      <th>plasma_glucose_conc</th>\n",
       "      <th>D_blood_pressure</th>\n",
       "      <th>skin_fold_thickness</th>\n",
       "      <th>serum_insulin</th>\n",
       "      <th>body_mass_index</th>\n",
       "      <th>pedigree_func</th>\n",
       "      <th>age</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_preg  plasma_glucose_conc  ...  age  diabetes\n",
       "0         6                  148  ...   50         1\n",
       "1         1                   85  ...   31         0\n",
       "2         8                  183  ...   32         1\n",
       "3         1                   89  ...   21         0\n",
       "4         0                  137  ...   33         1\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RbHhT_yMgh1p"
   },
   "source": [
    "**This dataset contains 13 columns and based on different features, it is guessed whether or not a person has Diabetes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67Eap_ejyyJ5"
   },
   "source": [
    "### Separating Input variables and output variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly3PQ9DayxWX"
   },
   "outputs": [],
   "source": [
    "X = diabetes_data.drop('diabetes', axis = 1)\n",
    "y = diabetes_data.diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOQVck62gh1z"
   },
   "source": [
    "#### Split into training and testing (80:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AylKRHMgh12"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# The below line of code will not need to separate input variables and output variables.\n",
    "# The code is very simple if you remember numpy and pandas session. Indexing dataframe and arrays\n",
    "# x_train, x_test, y_train, y_test = train_test_split(diabetes.iloc[:, :-1], diabetes.iloc[:,-1], test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P3R17-E7kqjD"
   },
   "source": [
    "**Note for learners:** Here we have used MLPClassifier from neural_network module of sklearn library. MLP Classifier is also a classification algorithm like logistic regression or decision tree. We will soon learn about Neural Networks and Artificial Neural Networks in the upcoming sessions. So, no need to worry about it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZizeBIHwgh17"
   },
   "source": [
    "### Developing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqeygkPIgh18"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "mlp.fit(x_train, y_train)\n",
    "y_pred = mlp.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8-k13W0gh1T"
   },
   "source": [
    "## Performance Evaluation\n",
    "Evaluating performance of the machine learning model that we have built is an essential part of any machine learning project. Performance of our model is done using some evaluation metrics. Accuracy score is one among them.\n",
    "\n",
    "**Why accuracy score is not a good evaluation metric?**\n",
    "\n",
    "Our model may give satisfying results if we use accuracy score for a particular dataset but at the same time accuracy score is not a good measure of evaluation for some particular dataset like fraud detection that we discussed during class imbalance problem. Let's consider the same dataset, suppose we have 1000 transaction in the dataset. Out of 1000 transactions 20 transactions are fraud transactions. Now let's say you build a model which predicted all the 1000 transactions as not fraud transaction, for 980 transaction which were not fraud, the prediction is correct while the transaction which were actually fraud are also predicted as not fraud. The accuracy is nothing but total correct prediction divided by total prediction. In this case we have total prediction as 1000 (as we have 1000 transactions) while total correct prediction is 980, resulting the accuracy score of 980/1000 = 0.98. The model is giving 98% of accuracy. Do you think the model is good? No, because our model could not notice the transaction which were actually fraud. \n",
    "\n",
    "Here we will discuss some other metrices for both classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1D9DMXXigh1Z"
   },
   "source": [
    "## 1. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD9O15axgh2G"
   },
   "source": [
    "**All performance metrics in sklearn are to be written in the same way -**  \n",
    "> ``` metric_function(true_label, predicted_labels) ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_V1Az3YsBUgP"
   },
   "source": [
    "Below are the metrics for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtZ2yI1Lgh2I"
   },
   "source": [
    "### Confusion Matrix\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.\n",
    "\n",
    "Further reading about confusion matrix and its related terminologies: \n",
    "1. https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "2. https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKr_nFuRgh2K"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "i3S3OpD6QLmq",
    "outputId": "d74917f4-9ab9-4fdb-ba1c-fc27e771726b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[87,  5],\n",
       "       [44, 18]])"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "12hGE_5rgh2O",
    "outputId": "d57ac6d2-d93c-4935-a032-7f57f35291cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive 18\n",
      "True Negative 87\n",
      "False Positive 5\n",
      "False Negative 44\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()     # ravel() is used to convert a 2D array to 1D array. The output by confusion matrix is a 2D array.\n",
    "print(\"True Positive\", tp)\n",
    "print(\"True Negative\", tn)\n",
    "print(\"False Positive\", fp)\n",
    "print(\"False Negative\", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SqjqAVSQgh2S"
   },
   "source": [
    "### Accuracy\n",
    "\\begin{align}\n",
    "Accuracy = \\frac{TP+TN}{TP+TN+FN+FP}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AX3DELNUgh2U",
    "outputId": "8e453fc6-9ee2-4e1e-9531-5967515e13aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6883116883116883"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNFBGQaCEjCA"
   },
   "source": [
    "**When is it good to use accuracy score as a model evaluation metric?**\n",
    "1. The classifications in the dataset is nearly symmetrical (means equal distribution of all the classes).\n",
    "2. The false positive and false negative on test data are nearly equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yh4fSgV9gh2a"
   },
   "source": [
    "### Recall (Sensitivity)\n",
    "\\begin{align}\n",
    "Sensitivity = \\frac{TP}{TP+FN}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGBdDw9hgh2b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Jk9ffog9gh2g",
    "outputId": "4f7fff70-efdc-4434-a8fc-84b4b0426f32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3387096774193548"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlQf5BUtgh2o"
   },
   "source": [
    "### Specificity\n",
    "sklearn does not have an inbuild function for Specificity. But by adding parameter pos_label =0 to the recall function, we treat that as the positive class, and hence gives the correct output\n",
    "\\begin{align}\n",
    "Specificity = \\frac{TN}{TN+FP}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lHTxK6NHgh2q",
    "outputId": "c257a68f-a910-48b9-aae5-f60cf7eee5a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity with recall pos label=0:  0.9239130434782609\n"
     ]
    }
   ],
   "source": [
    "print(\"Specificity with recall pos label=0: \",recall_score(y_test, y_pred, pos_label=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UAM0Kz2gh2u"
   },
   "source": [
    "**Checking with formulas (tn , fp from confusion matrix):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rrthV4dhgh2v",
    "outputId": "e14dadc7-ce97-4057-9014-8d60cb13752c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity with Formulas:  0.9239130434782609\n"
     ]
    }
   ],
   "source": [
    "print(\"Specificity with Formulas: \", tn/(tn+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7QrITQBgh21"
   },
   "source": [
    "They are the same! You can use either one of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plCwrQB_gh23"
   },
   "source": [
    "### Precision\n",
    "\\begin{align}\n",
    "Precision = \\frac{TP}{TP+FP}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "14pynqe6gh25",
    "outputId": "f0a0df1d-a9a2-4c64-adfe-8e5c271cdc07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_2Y9vQ9gh2_"
   },
   "source": [
    "### Imbalanced Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "UwMwh4mhgh3A",
    "outputId": "adcb9054-0209-410d-851f-c738ef8ecafb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: diabetes, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_data.iloc[:,-1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zY7eJaxsgh3K"
   },
   "source": [
    "### Matthews Correlation Coefficient\n",
    "\\begin{align}\n",
    "MCC = \\frac{(TP*TN)-(FP*FN)}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vr62zEHWgh3L",
    "outputId": "7c65053a-33a7-40f0-f7a2-a9044443ea3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC Score:  0.3339317909634408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "print(\"MCC Score: \",matthews_corrcoef(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hum7bVaAgh3P"
   },
   "source": [
    "### F1 Score\n",
    "It is the harmonic mean of Precision and recall\n",
    "\n",
    "\\begin{align}\n",
    "Precision = \\frac{2*Precision*Recall}{Precision+Recall}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q3A9uIKwgh3Q",
    "outputId": "1f755617-b37f-4bf3-e89e-acb7139b850d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 Score: \",f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JVcVsBEhgh3V"
   },
   "source": [
    "## Area Under the Curve (Reciever Operating Characterstics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YypfoTaVgh3X"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "ZZPwqn0Dgh3c",
    "outputId": "2d0c5375-77f5-4353-905e-3027bfcce8a4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_curve(mlp, x_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHrohQufgh3l"
   },
   "source": [
    "## 1.2 Regression Evaluation Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUc_2Fz7gh3m"
   },
   "source": [
    "Wine Dataset  \n",
    "    <b> Predictor Variable: </b> Quality (Tells quality of wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "WMZbaFOcgh3o",
    "outputId": "57a45b94-0f83-4c9c-d42d-940cff64867b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
       "0            7.0              0.27         0.36  ...       0.45      8.8        6\n",
       "1            6.3              0.30         0.34  ...       0.49      9.5        6\n",
       "2            8.1              0.28         0.40  ...       0.44     10.1        6\n",
       "3            7.2              0.23         0.32  ...       0.40      9.9        6\n",
       "4            7.2              0.23         0.32  ...       0.40      9.9        6\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/winequality.csv\", sep=\";\")\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GP8EUDXTgh3s"
   },
   "source": [
    "#### Split into training and testing (80:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGgMF3Rzgh3t"
   },
   "outputs": [],
   "source": [
    "# Here we are performing both separation of input and output variable and the splitting.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(wine.iloc[:, :-1], wine.iloc[:,-1], test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnuT2aaQgh3z"
   },
   "source": [
    "Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bsRa3Pe0gh32"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_PMAIOYRgh4B",
    "outputId": "3f3ef5c0-0fe0-4aa4-9b9b-afa1d2eb8f83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.44455619, 5.57868309, 5.99091469, 5.19864346, 6.0666099 ,\n",
       "       5.01639077, 5.68416174, 6.26611011, 5.97010538, 5.65519351])"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEQuSCzMgh4R"
   },
   "source": [
    "## Performance Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FZ86Xpbgh4U"
   },
   "source": [
    "Let y = Actual Value,  $\\tilde{y}$ = Predicted Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AjIBhaItgh4X"
   },
   "source": [
    "### Mean Absolute Error  \n",
    "*  MAE is the absolute difference between the target value and the value predicted by the model.  \n",
    "*  The MAE is more robust to outliers and does not penalize the errors as extremely as mse\n",
    "\\begin{align}\n",
    "MAE  = \\frac{1}{n}\\sum|y-\\tilde{y}| \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZNOb1yb2gh4Y",
    "outputId": "82acb1ee-cecd-4a11-9448-d4ba33fa9920"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5972358558776472"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHrz_JZAgh4j"
   },
   "source": [
    "### Mean Squared Error\n",
    "*  It is simply the average of the squared difference between the target value and the value predicted by the regression model. \n",
    "*  As it squares the differences, it penalizes even a small error which leads to over-estimation of how bad the model is.\n",
    "*  MSE or Mean Squared Error is one of the most preferred metrics for regression tasks. \n",
    "\\begin{align}\n",
    "MSE & = \\frac{1}{n}\\sum(y-\\tilde{y})^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NHZcMiDrgh4k",
    "outputId": "17e1926d-fe7e-470e-82e6-8394391af81f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  0.5906658099548077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Mean Squared Error: \",mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mx7Dtz5Wgh4r"
   },
   "source": [
    "### Root Mean Square Error\n",
    "*  RMSE is the square root of the averaged squared difference between the target value and the value predicted by the model. \n",
    "*  It is preferred more in some cases because the errors are first squared before averaging which poses a high penalty on large errors.\n",
    "*  This implies that RMSE is useful when large errors are undesired.\n",
    "\\begin{align}\n",
    "RMSE  = \\sqrt{\\frac{1}{n}\\sum(y-\\tilde{y})^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EfQI1JFzgh4u",
    "outputId": "f6299173-2152-489d-ca97-4ef038209300",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error:  0.7685478579469256\n",
      "Root Mean Squared Error:  0.7685478579469256\n"
     ]
    }
   ],
   "source": [
    "print(\"Root Mean Squared Error: \",mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Onetrj_Agh5A"
   },
   "source": [
    "### R Squared\n",
    "<li>The metric helps us to compare our current model with a constant baseline and tells us how much our model is better\n",
    "\\begin{align}\n",
    "R^2 = 1 - \\frac{MSE(Model)}{MSE(Baseline)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XmTpqG2rgh5B",
    "outputId": "5a02a6e2-e1b1-474e-8f79-92bb71c15dc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2832037191111023"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VoVhiVKgh5H"
   },
   "source": [
    "# 2. Cross Validation\n",
    "Usually, our data is divided into Train and Test Sets.\n",
    "The Train set is further divided into Train and Validation set. \n",
    "\n",
    "The Validation Set helps us in selecting good parameters/tune the parameters for our model.\n",
    "\n",
    "This Three fold set can be seen in the figure below:\n",
    "\n",
    "![train-test-val](https://amueller.github.io/ml-training-intro/slides/images/threefold_split.png)\n",
    "\n",
    "Our dataset should be as large as possible to train the model and removing considerable part of it for validation poses a problem of losing valuable portion of data that we would prefer to be able to train. \n",
    "\n",
    "In order to address this issue, we use the Cross validation technique. Cross Validation has a number of types out of which we'll be using K-fold cross validation today.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gESg0k8_gh5H"
   },
   "source": [
    "### 2.1 K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKX8Gjyugh5I"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7kl2Ah_gh5Q"
   },
   "source": [
    "<blockquote> We can also import cross_val_score from the same library, but it only allows a single scorer to be implemented. So we are using cross_validate </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "4GiU8TBWgh5Q",
    "outputId": "88628060-694c-4f07-a1a5-057d8fbc08f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.29621458, 0.58519506, 0.2758708 , 0.71810746, 0.45174599,\n",
       "        0.6917429 , 0.34141088, 0.3458221 , 0.43011928, 0.27908921]),\n",
       " 'score_time': array([0.00480342, 0.0043273 , 0.00410509, 0.00405145, 0.00408554,\n",
       "        0.00405836, 0.00369906, 0.00416088, 0.00384641, 0.00388813]),\n",
       " 'test_accuracy': array([0.64935065, 0.7012987 , 0.68831169, 0.72727273, 0.7012987 ,\n",
       "        0.7012987 , 0.75324675, 0.71428571, 0.75      , 0.69736842]),\n",
       " 'test_precision': array([0.5       , 0.66666667, 0.57142857, 0.6875    , 0.57142857,\n",
       "        1.        , 0.83333333, 0.56756757, 0.62068966, 0.61538462]),\n",
       " 'test_recall': array([0.7037037 , 0.2962963 , 0.44444444, 0.40740741, 0.59259259,\n",
       "        0.14814815, 0.37037037, 0.77777778, 0.69230769, 0.30769231])}"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = cross_validate(mlp, X, y, cv=10, scoring=[\"accuracy\", \"precision\", \"recall\"])\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C2N7yzQRgh5V"
   },
   "source": [
    "**cv=10 is provided, which means we are performing 10 fold cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jxN-rYmigh5W",
    "outputId": "544e8651-f146-41cd-cd79-1dbf3cab5961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7083732057416269\n",
      "Precision:  0.663399898098174\n",
      "Recall:  0.47407407407407404\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", cv_results[\"test_accuracy\"].mean())\n",
    "print(\"Precision: \", cv_results[\"test_precision\"].mean())\n",
    "print(\"Recall: \", cv_results[\"test_recall\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrNp0R93gh5a"
   },
   "source": [
    "**For all valid scoring options - use the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "7mz4G3U5gh5e",
    "outputId": "19733abb-f81e-4672-ba1b-f2e36b12f5da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as m\n",
    "m.SCORERS.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygp5tnDdgh5q"
   },
   "source": [
    "For more complicated scoring metrics (such as specificity, which isn't explicilty provided by sklearn), or to create your own metrics, \n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#using-multiple-metric-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YoVnDQsigh5x"
   },
   "source": [
    "### 2.2 Leave One Out Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDxbxH6ggh5x"
   },
   "source": [
    "<blockquote>This code takes a long time to run, you can either skip running this part and directly just see the printed results, or wait for 10-15 mins for this to run </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZEbpVaygh5y"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vBvtai8Xgh55",
    "outputId": "e6144369-f310-41f0-d090-82fdd547f27f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.50730586, 0.39103937, 0.3785851 , 0.34293866, 0.67757607,\n",
       "        0.45510221, 0.31566334, 0.3564589 , 0.59180951, 0.64962935,\n",
       "        0.79952979, 0.38669538, 0.37384248, 0.46679831, 0.31697583,\n",
       "        0.69750547, 0.38226819, 0.4034586 , 0.72357416, 0.51584172,\n",
       "        0.34693027, 0.40484476, 0.76911044, 0.44652915, 0.4016099 ,\n",
       "        0.45877767, 0.4892385 , 0.34868908, 0.48699975, 0.56012654,\n",
       "        0.28004527, 0.38736725, 0.46224761, 0.37005901, 0.56642556,\n",
       "        0.86166453, 0.42618465, 0.60612273, 0.56659317, 0.28077841,\n",
       "        0.51177859, 0.71510482, 0.49838495, 0.34566236, 0.96387982,\n",
       "        0.40084004, 0.39566731, 0.62272024, 0.58541226, 0.40983129,\n",
       "        0.63371897, 0.50266194, 0.50982666, 0.23233771, 0.45473289,\n",
       "        0.77129245, 0.33790708, 0.5541513 , 0.30765033, 0.58533669,\n",
       "        0.62194061, 0.36196113, 0.6268785 , 0.50821567, 0.50683355,\n",
       "        0.88323975, 0.40367079, 0.50237656, 0.42735982, 0.6504724 ,\n",
       "        0.38024497, 0.57250094, 0.64503789, 0.82746315, 0.51225972,\n",
       "        0.62388301, 0.44489169, 0.87477732, 0.52063894, 0.49461985,\n",
       "        0.44234705, 0.41675043, 0.5911231 , 0.45367765, 0.36274457,\n",
       "        0.31572509, 0.68789387, 0.40916586, 0.54134703, 0.27804065,\n",
       "        0.69607925, 0.62398815, 0.54078293, 0.31348586, 0.39026928,\n",
       "        0.34479952, 0.69779849, 0.27567315, 0.45785594, 0.75111723,\n",
       "        0.29828572, 0.63067961, 0.27890182, 0.77096033, 0.34319282,\n",
       "        0.41855359, 0.17844105, 0.55085397, 0.6166451 , 0.31970096,\n",
       "        0.87324333, 0.5991354 , 0.41491246, 0.27379632, 0.24299026,\n",
       "        0.74814296, 0.55430055, 0.60948658, 0.67595696, 0.86298013,\n",
       "        0.64194298, 0.58206558, 0.43669724, 0.33052778, 0.57572579,\n",
       "        0.3123908 , 0.32292223, 0.48008204, 0.50641227, 0.53309989,\n",
       "        0.53254104, 0.43763161, 0.56141305, 0.37600875, 0.43088937,\n",
       "        0.44715738, 0.2873199 , 0.40292239, 0.38188171, 0.80430937,\n",
       "        0.26758742, 0.49948406, 0.78018093, 0.6871376 , 0.48841763,\n",
       "        0.32165408, 0.97195125, 0.42114782, 0.39670229, 0.29362059,\n",
       "        0.3063333 , 0.57730341, 0.56444192, 0.25047088, 0.55816102,\n",
       "        0.49621725, 0.20990252, 0.49072027, 0.81003022, 0.56464791,\n",
       "        0.48202491, 0.43141675, 0.59102821, 0.30618429, 0.64509392,\n",
       "        0.82662058, 0.49485874, 0.46399879, 0.75833702, 0.76691151,\n",
       "        0.63236356, 0.61851239, 0.76055837, 0.33469605, 0.54722166,\n",
       "        0.35061097, 0.56533551, 0.39492869, 0.47618079, 0.68438935,\n",
       "        0.43808556, 0.25239158, 0.73235416, 0.31346464, 0.38547397,\n",
       "        0.75652933, 0.45650458, 0.59283566, 0.45001841, 0.49798036,\n",
       "        0.42023349, 0.50905728, 0.50886297, 0.30719328, 0.56281352,\n",
       "        0.71389055, 0.46402216, 0.40586495, 0.60686803, 0.74238014,\n",
       "        0.64801478, 0.79747224, 0.42937517, 0.41171408, 0.45035911,\n",
       "        0.6549809 , 0.51883364, 0.50456285, 0.27675509, 0.62942362,\n",
       "        0.50059199, 0.14946032, 0.76414132, 0.58609486, 0.60700011,\n",
       "        0.50814891, 0.55753517, 0.37092447, 0.43860364, 0.24928856,\n",
       "        0.36576271, 0.6262331 , 0.83892703, 0.50375032, 0.23077893,\n",
       "        0.36231923, 0.48704195, 0.25894523, 0.59857607, 0.48744583,\n",
       "        0.50843573, 0.23250961, 0.30746293, 0.52059793, 0.54861355,\n",
       "        0.49909568, 0.33797383, 0.19670296, 0.77712154, 0.25072885,\n",
       "        0.43755221, 0.37105823, 0.51000452, 0.61033463, 0.48814917,\n",
       "        0.41725993, 0.40461445, 0.41689396, 0.33611917, 0.38669658,\n",
       "        0.44154882, 0.27703404, 0.52403283, 0.66037607, 0.62762189,\n",
       "        0.3710773 , 0.61725163, 0.30788946, 0.368922  , 0.67218232,\n",
       "        0.49745011, 0.51531076, 0.32692504, 0.38342071, 0.29506731,\n",
       "        0.62057686, 0.48418021, 0.39111996, 0.36864638, 0.68620992,\n",
       "        0.32239056, 0.63960195, 0.95251751, 0.50222778, 0.61470985,\n",
       "        0.58875871, 0.71925592, 0.37200975, 0.54626894, 0.60649037,\n",
       "        0.31521583, 0.79248261, 0.3425045 , 0.46332383, 0.50363755,\n",
       "        0.35989189, 0.74384904, 0.43089223, 0.42081523, 0.45888448,\n",
       "        0.44502521, 0.33353019, 0.55650544, 0.44638038, 0.78658557,\n",
       "        0.49755287, 0.35301375, 0.69880962, 0.2718637 , 0.26369977,\n",
       "        0.4858892 , 0.61532903, 0.50645494, 0.59941983, 0.82268739,\n",
       "        0.52372217, 0.31543541, 0.42105961, 0.36198783, 0.37601066,\n",
       "        0.25075912, 0.45410252, 0.70465612, 0.43865848, 0.59968066,\n",
       "        0.44467163, 0.35018492, 0.51401281, 0.32321644, 0.30682516,\n",
       "        0.51369858, 0.25939393, 0.28081608, 0.38474655, 0.70008588,\n",
       "        0.33331513, 0.57876468, 0.21211481, 0.72450757, 0.95605063,\n",
       "        0.47700286, 0.26263976, 0.2472651 , 0.49544764, 0.24907517,\n",
       "        0.30414867, 0.34328294, 0.43865752, 0.44693375, 0.97893524,\n",
       "        0.56203175, 0.35311055, 0.58508515, 0.28500342, 0.36895633,\n",
       "        0.32483625, 0.31781936, 0.32554173, 0.53481317, 0.51134324,\n",
       "        0.61085176, 1.02217984, 0.43286133, 0.22543788, 0.5257926 ,\n",
       "        0.66589355, 0.31467938, 0.43292284, 0.45900297, 0.50305867,\n",
       "        0.48161197, 0.5481391 , 0.42079878, 0.57495832, 0.57227898,\n",
       "        0.54578614, 0.30687761, 0.65107441, 0.30479407, 0.56959867,\n",
       "        0.36357474, 0.25388432, 0.51051378, 0.28154397, 0.37497473,\n",
       "        0.31706595, 0.55828166, 0.48334193, 0.71841645, 0.45801878,\n",
       "        0.26609445, 0.59569454, 0.44561839, 0.45351195, 0.6018889 ,\n",
       "        0.44711733, 0.41875815, 0.70146799, 0.42199063, 0.67317152,\n",
       "        0.18649769, 0.66864896, 0.34339571, 0.35656071, 0.43449926,\n",
       "        0.38133502, 0.37107015, 0.49314165, 0.58893275, 0.2436471 ,\n",
       "        0.33332443, 0.5997262 , 0.69046211, 0.62640738, 0.42875743,\n",
       "        0.46874118, 1.08774304, 0.33255839, 0.40647817, 0.35167408,\n",
       "        0.61131406, 0.55270839, 0.87914586, 0.25186753, 0.56511378,\n",
       "        0.5122416 , 0.57520986, 0.30751348, 0.92692351, 0.38100219,\n",
       "        0.33869648, 0.72362614, 0.65718865, 0.25596404, 0.35867643,\n",
       "        0.5180738 , 0.4628458 , 0.50547576, 0.24320269, 0.27857685,\n",
       "        0.62122488, 0.48932171, 0.40939784, 0.54383969, 0.53395247,\n",
       "        0.82250166, 0.72553897, 0.42030025, 0.55875111, 0.38659358,\n",
       "        0.61839128, 0.46889019, 0.50562644, 0.52584624, 0.4758029 ,\n",
       "        0.42589307, 0.3759923 , 0.51504707, 0.51852775, 0.68513417,\n",
       "        0.3309052 , 0.52296805, 0.40278077, 0.52860427, 0.44890475,\n",
       "        0.73085928, 0.359375  , 0.52518964, 0.75684977, 0.54009771,\n",
       "        0.53527355, 0.51880932, 0.60084105, 0.54684639, 0.60830641,\n",
       "        0.31442094, 0.26865578, 0.22748637, 0.47475886, 0.56038213,\n",
       "        0.65668488, 1.0460217 , 0.26534772, 0.56569171, 0.60324502,\n",
       "        0.84195137, 0.42729378, 0.37988257, 0.54952765, 0.2602489 ,\n",
       "        0.82531643, 0.58734155, 0.41226411, 0.60308766, 0.28513288,\n",
       "        0.6555593 , 0.57859397, 0.72948909, 0.3710885 , 0.46684003,\n",
       "        0.65351462, 0.46801805, 0.33993411, 0.49638057, 0.43207169,\n",
       "        0.41521955, 0.62563372, 0.51783466, 1.11161733, 0.61885118,\n",
       "        0.62488151, 0.39372802, 0.44801497, 0.30019617, 0.45743299,\n",
       "        0.32074165, 0.6057539 , 0.97192812, 0.46601915, 0.44294024,\n",
       "        0.62417483, 0.20913672, 0.47826767, 0.41153669, 0.87734222,\n",
       "        0.53514171, 0.25703073, 0.59059811, 0.51224327, 0.53991437,\n",
       "        0.5196619 , 0.3775382 , 0.5793736 , 0.43826866, 0.46364427,\n",
       "        0.537117  , 0.39658785, 0.94859099, 0.27196002, 0.25691009,\n",
       "        0.65624547, 0.92491817, 0.42414045, 0.71806431, 0.44650698,\n",
       "        0.4644599 , 0.39326262, 0.29394555, 0.88810468, 0.70407152,\n",
       "        0.31758308, 0.55566859, 0.44598293, 0.44850516, 0.33450389,\n",
       "        0.40363312, 0.2563808 , 0.3209362 , 0.36650252, 0.56945252,\n",
       "        0.40718579, 0.27715802, 0.28635859, 0.34388041, 0.45333982,\n",
       "        0.68478298, 0.44086146, 0.3939147 , 0.45775056, 0.93447423,\n",
       "        0.90768385, 0.83178592, 0.39524221, 0.6199646 , 0.35169244,\n",
       "        0.33078361, 0.35810184, 0.84987569, 0.35482359, 0.68037558,\n",
       "        0.47473454, 0.78890491, 0.45332432, 0.59973884, 0.5827713 ,\n",
       "        0.48593903, 0.38549614, 0.55757856, 0.3684876 , 0.70169735,\n",
       "        0.39629841, 0.54793859, 0.40167665, 0.4215889 , 0.52615881,\n",
       "        0.38079691, 0.52632761, 0.44794536, 0.53375292, 0.34563065,\n",
       "        0.34101582, 0.42072964, 0.64180064, 0.24822593, 0.68584967,\n",
       "        0.64121628, 0.29628849, 0.6263957 , 0.51207614, 0.6671443 ,\n",
       "        0.29216599, 0.38482285, 0.36412525, 0.56165886, 0.56921315,\n",
       "        0.27161312, 0.43744254, 0.67779136, 0.5459795 , 0.39110637,\n",
       "        0.44673824, 0.54915643, 0.62312555, 0.43569183, 0.53763771,\n",
       "        0.37282491, 0.56465149, 0.49840307, 0.6091156 , 0.55153823,\n",
       "        0.78704262, 0.61454463, 0.92076707, 0.87946177, 1.10402942,\n",
       "        0.65405703, 0.95089793, 1.38472152, 1.13192487, 1.40933418,\n",
       "        0.92431974, 1.60535908, 1.15412736, 0.55259299, 0.32905436,\n",
       "        0.35290551, 0.55761623, 0.68772769, 0.52233052, 0.82913446,\n",
       "        0.86952925, 0.27097392, 0.20929837, 0.32807541, 0.84722948,\n",
       "        0.43062472, 0.67721915, 0.65864468, 0.65829182, 0.64823341,\n",
       "        0.68727016, 0.30998611, 0.56258249, 0.34317708, 0.36394167,\n",
       "        0.43214321, 0.33515334, 0.89036584, 0.49955678, 0.4777596 ,\n",
       "        0.61316705, 0.90435982, 0.43014526, 0.67675805, 0.45163298,\n",
       "        0.3952775 , 0.38203526, 0.51322222, 0.74476695, 0.44127774,\n",
       "        0.79073286, 0.69823337, 0.28499484, 0.56968617, 0.43776846,\n",
       "        0.30326676, 0.50333905, 0.3099544 , 0.3387289 , 0.61779308,\n",
       "        0.51024199, 0.23256254, 0.51197863, 0.32010436, 0.45241046,\n",
       "        0.46243334, 0.53936982, 0.75128508, 0.25017214, 0.53436303,\n",
       "        0.27920246, 0.34990597, 0.5100584 , 0.33156848, 0.34704375,\n",
       "        0.38700843, 0.22534871, 0.4167738 , 0.39090204, 0.66232443,\n",
       "        0.75681567, 0.36293888, 0.35363173, 0.36528397, 0.49612546,\n",
       "        0.33282375, 0.34284687, 0.80196834, 0.20854759, 0.27977562,\n",
       "        0.55182481, 0.6222415 , 0.74293113, 0.69568133, 0.40939522,\n",
       "        0.41072559, 0.72633553, 0.34529471, 0.45821595, 0.78345156,\n",
       "        0.50869346, 0.73004174, 0.28606701, 0.51381993, 0.45058227,\n",
       "        0.5762291 , 0.65905762, 0.5139811 , 0.39514589, 0.5609746 ,\n",
       "        0.61803222, 0.75576353, 0.47945166, 0.32181883, 0.37864971,\n",
       "        0.60092211, 0.55325818, 0.58507299, 1.04618669, 0.4217937 ,\n",
       "        0.43603182, 0.47723913, 0.54532003, 0.65080047, 0.44284749,\n",
       "        0.48901558, 0.67229056, 0.65569568, 0.44925737, 0.48109078,\n",
       "        0.35198069, 0.30822539, 0.40788245, 0.17979026, 0.658952  ,\n",
       "        0.26750445, 0.78686118, 0.4607234 , 0.62958574, 0.19917965,\n",
       "        0.45422101, 0.444911  , 0.7776618 , 0.62228775, 0.65484333,\n",
       "        0.65521836, 1.01713729, 0.64643264]),\n",
       " 'score_time': array([0.00173497, 0.00175524, 0.00165486, 0.00155067, 0.00159764,\n",
       "        0.00186968, 0.0018754 , 0.00180984, 0.00173616, 0.00187302,\n",
       "        0.00188565, 0.00183892, 0.00177193, 0.00172758, 0.00181389,\n",
       "        0.00171685, 0.00181818, 0.00162911, 0.00175858, 0.00172687,\n",
       "        0.00170469, 0.0016923 , 0.001683  , 0.0017004 , 0.00169683,\n",
       "        0.00192165, 0.00177789, 0.00162315, 0.00171518, 0.00177479,\n",
       "        0.00188994, 0.00179315, 0.00164962, 0.00178123, 0.00175357,\n",
       "        0.00169659, 0.00148749, 0.00163126, 0.00166702, 0.00181246,\n",
       "        0.0019691 , 0.00176048, 0.00167179, 0.00178695, 0.00164151,\n",
       "        0.0018065 , 0.00181651, 0.00172472, 0.0017066 , 0.00163627,\n",
       "        0.00183034, 0.0018065 , 0.00177813, 0.00173426, 0.00174165,\n",
       "        0.0016551 , 0.00169802, 0.00188208, 0.00173545, 0.00179577,\n",
       "        0.00186253, 0.0016458 , 0.00160813, 0.00191331, 0.00179148,\n",
       "        0.00254512, 0.00174165, 0.00181985, 0.00173497, 0.00169611,\n",
       "        0.00171518, 0.0015192 , 0.0016737 , 0.0019052 , 0.00162959,\n",
       "        0.00177836, 0.0018611 , 0.0018394 , 0.00176144, 0.00181651,\n",
       "        0.00178456, 0.00168943, 0.00184059, 0.00186253, 0.00193572,\n",
       "        0.00171924, 0.00171542, 0.00194407, 0.00170875, 0.00171399,\n",
       "        0.00182247, 0.0016396 , 0.00172949, 0.00179744, 0.0017767 ,\n",
       "        0.001719  , 0.0034256 , 0.00182748, 0.00178337, 0.00180578,\n",
       "        0.00183702, 0.00176239, 0.0017767 , 0.00186062, 0.00362682,\n",
       "        0.00176024, 0.00178719, 0.00175643, 0.00186157, 0.00177908,\n",
       "        0.00169444, 0.00173974, 0.00174689, 0.00176072, 0.00170732,\n",
       "        0.00175738, 0.00182652, 0.00186276, 0.00171041, 0.00182438,\n",
       "        0.00172424, 0.0016613 , 0.00174427, 0.00181508, 0.00189614,\n",
       "        0.00185323, 0.00171161, 0.00180435, 0.0019846 , 0.00179815,\n",
       "        0.00172281, 0.00173068, 0.00183725, 0.00175071, 0.00186419,\n",
       "        0.00160909, 0.00177264, 0.00171328, 0.00178003, 0.00178409,\n",
       "        0.00171304, 0.00184107, 0.00173926, 0.00179029, 0.00177503,\n",
       "        0.00186419, 0.00180697, 0.00177073, 0.00172043, 0.00187969,\n",
       "        0.001863  , 0.00181127, 0.00176716, 0.00181365, 0.00181484,\n",
       "        0.00183105, 0.00180769, 0.00174332, 0.00184727, 0.00178504,\n",
       "        0.00175309, 0.00176883, 0.00192642, 0.0017519 , 0.00176191,\n",
       "        0.00180387, 0.00180292, 0.00180888, 0.00182438, 0.00178003,\n",
       "        0.00179648, 0.00170207, 0.00181675, 0.00176549, 0.00185561,\n",
       "        0.00181818, 0.00184488, 0.00190902, 0.00178051, 0.00173044,\n",
       "        0.0018785 , 0.00163937, 0.00172257, 0.00170112, 0.00189161,\n",
       "        0.00175309, 0.00177646, 0.00179577, 0.00182605, 0.00163174,\n",
       "        0.00199103, 0.00183749, 0.00187516, 0.00171924, 0.00179291,\n",
       "        0.00183415, 0.00182939, 0.00176501, 0.00176072, 0.00178337,\n",
       "        0.00186396, 0.00168371, 0.00167942, 0.00167942, 0.00177121,\n",
       "        0.00181174, 0.00171065, 0.00191712, 0.00195765, 0.00176167,\n",
       "        0.00177026, 0.00166965, 0.00179935, 0.0017612 , 0.0018115 ,\n",
       "        0.00179338, 0.00177479, 0.00179935, 0.00190449, 0.00196958,\n",
       "        0.0017252 , 0.00186396, 0.00174451, 0.00181198, 0.00173354,\n",
       "        0.00175214, 0.00189972, 0.00171232, 0.00171232, 0.00168347,\n",
       "        0.00172544, 0.00173616, 0.00201201, 0.00166845, 0.0017519 ,\n",
       "        0.00176954, 0.00183463, 0.0016005 , 0.00175405, 0.00169778,\n",
       "        0.00167179, 0.00176382, 0.00188446, 0.0016582 , 0.00199366,\n",
       "        0.00192428, 0.00171232, 0.00178242, 0.00175762, 0.00174379,\n",
       "        0.00172472, 0.00175428, 0.00184464, 0.00170279, 0.00180578,\n",
       "        0.00175786, 0.00177121, 0.00177789, 0.00168133, 0.00169659,\n",
       "        0.00176573, 0.00180507, 0.00170755, 0.00200748, 0.00174785,\n",
       "        0.00180197, 0.00176263, 0.00169873, 0.0017302 , 0.00196552,\n",
       "        0.00182962, 0.00170374, 0.00189137, 0.00170493, 0.00180316,\n",
       "        0.00174832, 0.00173378, 0.0017283 , 0.0016439 , 0.00181341,\n",
       "        0.00178194, 0.00188684, 0.00174141, 0.0017705 , 0.00173545,\n",
       "        0.00180697, 0.00177741, 0.00173235, 0.00183845, 0.0017662 ,\n",
       "        0.00179243, 0.00175071, 0.00178719, 0.00177956, 0.00169539,\n",
       "        0.00172973, 0.00170732, 0.00174046, 0.00196075, 0.00176001,\n",
       "        0.00167441, 0.00172687, 0.00174069, 0.00200081, 0.00182843,\n",
       "        0.00189734, 0.00182033, 0.00169277, 0.00175738, 0.0017271 ,\n",
       "        0.0017128 , 0.00176096, 0.00191188, 0.00178242, 0.00187349,\n",
       "        0.00174308, 0.00160813, 0.0018146 , 0.0019238 , 0.00168371,\n",
       "        0.00188661, 0.00186896, 0.0019381 , 0.00179172, 0.00179386,\n",
       "        0.00187802, 0.00176644, 0.00179172, 0.0017662 , 0.0018692 ,\n",
       "        0.00203395, 0.00172973, 0.00177073, 0.00176692, 0.00175571,\n",
       "        0.00176382, 0.00164032, 0.00176311, 0.00182843, 0.00181246,\n",
       "        0.00178862, 0.00174093, 0.0017724 , 0.00180793, 0.001719  ,\n",
       "        0.00166368, 0.00166106, 0.00156736, 0.00170779, 0.00167179,\n",
       "        0.00178933, 0.00193691, 0.00178289, 0.00166798, 0.00172544,\n",
       "        0.00172138, 0.0022254 , 0.00179601, 0.00178409, 0.00174189,\n",
       "        0.00176859, 0.0017457 , 0.00183344, 0.00174594, 0.00178695,\n",
       "        0.00183296, 0.00180745, 0.00174379, 0.00170112, 0.00175571,\n",
       "        0.00172901, 0.0018084 , 0.00174046, 0.00179124, 0.00170493,\n",
       "        0.00173163, 0.00195909, 0.00183487, 0.00184751, 0.00178885,\n",
       "        0.00166535, 0.00174665, 0.00175261, 0.00166011, 0.00501084,\n",
       "        0.0017221 , 0.00172424, 0.00174928, 0.00166845, 0.00171137,\n",
       "        0.00162983, 0.00167274, 0.00174403, 0.0017333 , 0.00171089,\n",
       "        0.00182104, 0.00182414, 0.00174809, 0.00171423, 0.00168681,\n",
       "        0.00175023, 0.00176239, 0.00185966, 0.00165486, 0.00158691,\n",
       "        0.00171733, 0.00168204, 0.00170517, 0.00173926, 0.0016036 ,\n",
       "        0.00186777, 0.0017705 , 0.00168943, 0.00177455, 0.00169945,\n",
       "        0.00191379, 0.00167203, 0.00179911, 0.00180387, 0.00174665,\n",
       "        0.0017848 , 0.00189137, 0.00192451, 0.00187373, 0.00169635,\n",
       "        0.00176835, 0.0016942 , 0.00185561, 0.00188422, 0.00179696,\n",
       "        0.00166368, 0.00194359, 0.00186276, 0.00174904, 0.00177979,\n",
       "        0.00179601, 0.0017848 , 0.00174451, 0.00178027, 0.00176573,\n",
       "        0.00187254, 0.00177741, 0.00193214, 0.00180411, 0.00180578,\n",
       "        0.00216818, 0.00172281, 0.0018847 , 0.00183654, 0.00177073,\n",
       "        0.00170851, 0.00173426, 0.00186062, 0.00162268, 0.00156713,\n",
       "        0.00180721, 0.00173068, 0.00184488, 0.001683  , 0.00176454,\n",
       "        0.00184631, 0.00174642, 0.00185418, 0.00196719, 0.00170493,\n",
       "        0.00167727, 0.00172019, 0.00167131, 0.00183678, 0.00191522,\n",
       "        0.00174475, 0.00180626, 0.00187588, 0.00190067, 0.00183392,\n",
       "        0.00171995, 0.00177336, 0.00179172, 0.00175691, 0.0018332 ,\n",
       "        0.00188208, 0.00191784, 0.00179291, 0.00183821, 0.00200438,\n",
       "        0.00179625, 0.00179052, 0.00189471, 0.00179863, 0.00204945,\n",
       "        0.00178576, 0.00190711, 0.00177407, 0.00210857, 0.00185394,\n",
       "        0.00189781, 0.00162315, 0.00172091, 0.00193143, 0.00175548,\n",
       "        0.00167036, 0.00167108, 0.00202441, 0.00191879, 0.00182605,\n",
       "        0.00189614, 0.00190258, 0.0017941 , 0.00173664, 0.00180387,\n",
       "        0.00173473, 0.0017066 , 0.00199437, 0.00183272, 0.00191855,\n",
       "        0.00185776, 0.00183082, 0.00178313, 0.00183463, 0.00185466,\n",
       "        0.00182271, 0.0018363 , 0.00184631, 0.00180316, 0.00183606,\n",
       "        0.00181961, 0.00175333, 0.0017271 , 0.00182939, 0.00187802,\n",
       "        0.0019033 , 0.00176525, 0.00167012, 0.00177813, 0.00189042,\n",
       "        0.00184107, 0.00181055, 0.00183725, 0.00172734, 0.00221562,\n",
       "        0.00167108, 0.00180459, 0.00179386, 0.00168753, 0.00160575,\n",
       "        0.00172019, 0.00170827, 0.00174737, 0.00177503, 0.00169921,\n",
       "        0.00167704, 0.00178099, 0.0017705 , 0.00180316, 0.00171614,\n",
       "        0.00175762, 0.00198555, 0.00197554, 0.00212145, 0.00192142,\n",
       "        0.00186348, 0.00174546, 0.00166345, 0.00183415, 0.00179648,\n",
       "        0.00184464, 0.00169587, 0.00171757, 0.00186062, 0.00187111,\n",
       "        0.00197577, 0.00176907, 0.00190854, 0.00183892, 0.00180507,\n",
       "        0.00174689, 0.00175381, 0.00185013, 0.00179625, 0.00178289,\n",
       "        0.00180984, 0.00172639, 0.00165844, 0.0018599 , 0.00166845,\n",
       "        0.00172448, 0.00287032, 0.00176525, 0.00180125, 0.00174952,\n",
       "        0.00171161, 0.00174427, 0.00169802, 0.00167966, 0.00188756,\n",
       "        0.0018456 , 0.0017066 , 0.00186396, 0.00184011, 0.00180292,\n",
       "        0.00178862, 0.00163841, 0.0017488 , 0.00166535, 0.00172424,\n",
       "        0.00178266, 0.00185347, 0.00178933, 0.00171685, 0.00179648,\n",
       "        0.00183177, 0.00197458, 0.00174403, 0.00171852, 0.00182319,\n",
       "        0.00186658, 0.00178742, 0.00177622, 0.00178838, 0.00169516,\n",
       "        0.00177646, 0.00174212, 0.00183082, 0.00193501, 0.0039494 ,\n",
       "        0.00184393, 0.00185108, 0.00352192, 0.00196576, 0.00183558,\n",
       "        0.00176024, 0.00228548, 0.00189495, 0.00178623, 0.00181389,\n",
       "        0.0017643 , 0.00177789, 0.00195241, 0.00178313, 0.00173116,\n",
       "        0.00202036, 0.00183487, 0.00188375, 0.00184751, 0.00174332,\n",
       "        0.0017314 , 0.00185275, 0.00177956, 0.00182748, 0.00183558,\n",
       "        0.0018096 , 0.00183702, 0.00200987, 0.00179029, 0.00179458,\n",
       "        0.00174403, 0.00176001, 0.00168967, 0.00208688, 0.00174713,\n",
       "        0.00167894, 0.00172305, 0.00187016, 0.00190353, 0.00193429,\n",
       "        0.00167561, 0.00187469, 0.00174332, 0.00172949, 0.00177932,\n",
       "        0.00188971, 0.00186419, 0.0017662 , 0.00174022, 0.00183702,\n",
       "        0.00187182, 0.00191975, 0.00167108, 0.00171566, 0.00183058,\n",
       "        0.00186133, 0.00186396, 0.00191474, 0.00174618, 0.00172544,\n",
       "        0.00177789, 0.0017581 , 0.00185966, 0.00188231, 0.00166178,\n",
       "        0.00192356, 0.00180268, 0.00179291, 0.00181508, 0.00179362,\n",
       "        0.00173807, 0.00179863, 0.00180674, 0.00182676, 0.0018034 ,\n",
       "        0.00174618, 0.00182557, 0.00176167, 0.00178576, 0.00171423,\n",
       "        0.0018003 , 0.00181365, 0.00183988, 0.00171733, 0.0017643 ,\n",
       "        0.00190902, 0.00165796, 0.00193644, 0.00175142, 0.00205255,\n",
       "        0.00171804, 0.00173211, 0.00185275, 0.00177503, 0.00187898,\n",
       "        0.00189686, 0.00185847, 0.00186133, 0.00172734, 0.0017972 ,\n",
       "        0.00177765, 0.00180268, 0.00181985, 0.0018661 , 0.00171781,\n",
       "        0.0018301 , 0.00184965, 0.00181174, 0.00189042, 0.00158882,\n",
       "        0.00164962, 0.00177813, 0.00174189, 0.00184488, 0.00176859,\n",
       "        0.00179338, 0.0018518 , 0.00177455, 0.00168633, 0.00198197,\n",
       "        0.00181174, 0.00193787, 0.00174356, 0.00174284, 0.00171423,\n",
       "        0.00177622, 0.00177455, 0.00177479, 0.00179625, 0.00166583,\n",
       "        0.00181079, 0.00169516, 0.00173354, 0.00180221, 0.00173593,\n",
       "        0.00176167, 0.00195479, 0.00183725, 0.00170088, 0.0017488 ,\n",
       "        0.00175023, 0.00173736, 0.00175309]),\n",
       " 'test_accuracy': array([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 1.])}"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = cross_validate(mlp, X, y,\n",
    "                            cv=LeaveOneOut(), scoring=[\"accuracy\"])\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5VXdvPbqgh6F",
    "outputId": "d2ce70fc-554d-4860-d22b-119029069980"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7044270833333334"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test_accuracy'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oh7CwmWygh6b"
   },
   "source": [
    "We have not included precision and recall in the metrics here. Can you think why?  \n",
    "**<mark>Hint:</mark> Imagine the confusion matrix when the testing has only one sample**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1GyHfC0gh6j"
   },
   "source": [
    "## 3. Hyperparameter Tuning\n",
    "Hyperparameters are important parts of the ML model and can make the model gold or trash. Here we have discussed one of the popular hyperparameter tunning method i.e. using Grid Search CV. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEHcCWcegh6l"
   },
   "source": [
    "## 3.1 Grid Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQwxdMUkgh6m"
   },
   "source": [
    "### 3.1.1 Crime Rate- Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qVEuJimgh6n"
   },
   "source": [
    "**Predictor Variable: Crime Rate (Regression Based)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Wbh15ZTgh6p"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NamrnJjOgh6y"
   },
   "outputs": [],
   "source": [
    "crime = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/ML_Models/master/Performance_Evaluation/Standard%20Metropolitan%20Areas%20Data%20-%20train_data.csv\")\n",
    "train, test = train_test_split(crime)\n",
    "x_train = train.iloc[:,:-1]     # one can also do train.drop('crime_rate', axis = 1)\n",
    "y_train = train.crime_rate      # or np.array(train.crime_rate.values).reshape(len(x_train),1)\n",
    "x_test = test.iloc[:,:-1]       # or test.drop('crime_rate', axis = 1)\n",
    "y_test = test.crime_rate        # np.array(test.crime_rate.values).reshape(len(x_test),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DZvhZ0fgh66"
   },
   "source": [
    "Performance without grid search: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TjevVRegh67"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "u2rzRQoBgh7D",
    "outputId": "ae3653e5-01e7-4588-c44a-4b5895623e7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.433765668815223"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "no8Vw1Qmgh7N"
   },
   "source": [
    "Performance with Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UPswE7Ogh7O"
   },
   "source": [
    "**Step 1:** Define a parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6084pRAvgh7P"
   },
   "outputs": [],
   "source": [
    "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False], 'n_jobs':[-1,1,10,15]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPQl8cdHgh7Y"
   },
   "source": [
    "**Step 2:** Fit the model to find the best hyperparameters on training data, and select the scorer you want to select to optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "TSWCh9rDgh7Z",
    "outputId": "ba746062-6e48-45e4-e35a-a2aa5a92978e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=LinearRegression(copy_X=True, fit_intercept=True,\n",
       "                                        n_jobs=None, normalize=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'copy_X': [True, False],\n",
       "                         'fit_intercept': [True, False],\n",
       "                         'n_jobs': [-1, 1, 10, 15],\n",
       "                         'normalize': [True, False]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(lr,parameters, cv=3)\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTdZBb40gh7d"
   },
   "source": [
    "**Step 3:** Print the best obtained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mIonXbIogh7f",
    "outputId": "ae337f3e-ee27-474b-94fd-ee1002925ed4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-Urv2jtngh7n",
    "outputId": "6c97fa79-6ddd-4e64-8141-ae45cb3e083f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.433765668815223"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lr = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False)\n",
    "grid_lr.fit(x_train, y_train)\n",
    "y_pred= grid_lr.predict(x_test)\n",
    "mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gwhEjnmGgh7v"
   },
   "source": [
    "**Performance does not vary that much!**\n",
    "\n",
    "The number of hyperparameters for Linear Regression is very less. Hence all of them give similar performance (in this specific dataset)\n",
    "\n",
    "Let us try another parameter for which the performance varies a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1gp-Po0wgh7w"
   },
   "source": [
    "### 3.1.2 Artificial Neural Network\n",
    "In Linear Regression, there are not many parameters to optimise, hence performance may not vary that much. In many other classifiers, there are a number of hyper parameters to tune, so let us see an example of how performance is improved using Grid Search. We take an example of **Artificial Neural Networks.**\n",
    "\n",
    "You need not understand the working behind ANN, so it is okay if you do not understand the parameter grid in detail. Let's just see how the performance improves by applying Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K28nx24bNTMw"
   },
   "outputs": [],
   "source": [
    "# Use diabetes data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2W1Ei3Ugh7y"
   },
   "source": [
    "**Step 1:** Define a parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SX_8tDiSgh7z"
   },
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kcl_8hc6gh79"
   },
   "source": [
    "**Step 2:** Fit the model to find the best hyperparameters on training data, and select the scorer you want to select to optimise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kc9VN84Ygh7-"
   },
   "source": [
    "<blockquote> <i>  This code takes a long time to run, you can either skip running this part and directly just see the printed results, or wait for 10-15 mins for this to run </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "EBeiT6D1gh7_",
    "outputId": "2e49e8ed-7aaf-4c22-d343-538d879cde01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_fun=15000,\n",
       "                                     max_iter=1000, momentum=0.9,\n",
       "                                     n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     random_s...\n",
       "                                     validation_fraction=0.1, verbose=False,\n",
       "                                     warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'alpha': [0.0001, 0.05],\n",
       "                         'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_random = GridSearchCV(mlp, parameter_space, scoring = 'accuracy')\n",
    "mlp_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8f-duE9gh8J"
   },
   "source": [
    "**Step 3:** Print the best obtained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "V4bj3qClgh8K",
    "outputId": "a84f23b6-cf31-47e7-ba1a-83a58b7b25d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'hidden_layer_sizes': (50, 50, 50),\n",
       " 'learning_rate': 'constant',\n",
       " 'solver': 'adam'}"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmHmS7Nfgh8S"
   },
   "source": [
    "**Step 4:** Train your model on these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2repqpLogh8U"
   },
   "outputs": [],
   "source": [
    "mlp_grid = MLPClassifier(solver='adam', learning_rate='constant', hidden_layer_sizes=(100,), alpha=0.0001, \n",
    "                         activation='tanh',max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCXtKB2Ugh8a"
   },
   "outputs": [],
   "source": [
    "mlp_grid.fit(x_train, y_train)\n",
    "y_pred = mlp_grid.predict(x_test)\n",
    "acc_tuned = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jZTgmXQogh8e"
   },
   "source": [
    "**Comparing with Accuracy from model without hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "H-kojEW-gh8f",
    "outputId": "c4a7a755-586d-418c-9e6d-e55e640a84be",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Tuned model:  0.695\n",
      "Accuracy of non-Tuned model:  0.688\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Tuned model: \",np.round(acc_tuned,3))\n",
    "print(\"Accuracy of non-Tuned model: \",np.round(acc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HsGwxzNOgh8h"
   },
   "source": [
    "Approximately 5% difference in accuracy!  \n",
    "By including an even more exhaustive grid search, we can improve the performance even further"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HtZ2yI1Lgh2I",
    "SqjqAVSQgh2S",
    "Yh4fSgV9gh2a",
    "WlQf5BUtgh2o",
    "plCwrQB_gh23",
    "1_2Y9vQ9gh2_",
    "zY7eJaxsgh3K",
    "hum7bVaAgh3P",
    "GP8EUDXTgh3s",
    "AjIBhaItgh4X",
    "RHrz_JZAgh4j",
    "mx7Dtz5Wgh4r",
    "Onetrj_Agh5A",
    "gESg0k8_gh5H",
    "YoVnDQsigh5x",
    "QQwxdMUkgh6m",
    "1gp-Po0wgh7w"
   ],
   "name": "Performance Evaluation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
