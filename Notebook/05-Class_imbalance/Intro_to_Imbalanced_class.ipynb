{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tG5pZlm51LFg"
   },
   "source": [
    "**Author:** Baharat Ram Ammu | Manish KC\n",
    "\n",
    "**Contributor:** Chanukya Patnaik | Gunnika Batra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jn2WK-EnvdaQ"
   },
   "source": [
    "## Introduction\n",
    "The dataset used in this notebook is of '[IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/data)'. This notebook will introduce you to class imbalance problem.\n",
    "\n",
    "Data set link: [Fraud Dataset](https://drive.google.com/file/d/1q8SYcjOJULdSkETv5S_gd7xNq1GrBHAO/view)\n",
    "\n",
    "#### Imbalanced Problem\n",
    "Imbalanced classes are a common problem in machine learning classification where there are a disproportionate ratio of observations in each class. Class imbalance can be found in many different areas including medical diagnosis, spam filtering, and fraud detection.\n",
    "\n",
    "#### Agenda\n",
    "*  Loading Libraries\n",
    "*  Loading Data\n",
    "*  Getting Basic Idea About Data\n",
    "*  Missing Values and Dealing with Missing Values\n",
    "*  One Hot Encoding (Creating dummies for categorical columns)\n",
    "*  Standardization / Normalization\n",
    "*  Splitting the dataset into train and test data\n",
    "*  Dealing with Imbalanced Data\n",
    "    *  Resampling Techniques - Oversample Minority Class\n",
    "    *  Resampling Techniques - Undersample Majority Class\n",
    "    *  Generate Synthetic Samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89IQeG9VuUpM"
   },
   "source": [
    "## Loading Libraries\n",
    "All Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n",
    "\n",
    "In data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processin and data frames. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_lWWBjGvNP1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd                  # A fundamental package for linear algebra and multidimensional arrays\n",
    "import numpy as np                   # Data analysis and data manipulating tool\n",
    "import random                        # Library to generate random numbers\n",
    "from collections import Counter      # Collection is a Python module that implements specialized container datatypes providing \n",
    "                                     # alternatives to Python’s general purpose built-in containers, dict, list, set, and tuple.\n",
    "                                     # Counter is a dict subclass for counting hashable objects\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To ignore warnings in the notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jIwY1OGvr7A"
   },
   "source": [
    "## Loading Data\n",
    "Pandas module is used for reading files. We have our data in '.csv' format. We will use 'read_csv()' function for loading the data.\n",
    "\n",
    "**Disclaimer:** Loading fraud data will take time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_bYGjBvvkVv"
   },
   "outputs": [],
   "source": [
    "fraud_data = pd.read_csv(\"fraud_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kvyUn8YG8wyC"
   },
   "source": [
    "### Getting Basic Idea About Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "8CmggKjE8ZUk",
    "outputId": "7edc1219-5acf-4317-c1ad-18ed7f17d4c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2994681</td>\n",
       "      <td>0</td>\n",
       "      <td>242834</td>\n",
       "      <td>25.000</td>\n",
       "      <td>H</td>\n",
       "      <td>9803</td>\n",
       "      <td>583.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>...</td>\n",
       "      <td>firefox 56.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1920x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>desktop</td>\n",
       "      <td>rv:56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3557242</td>\n",
       "      <td>0</td>\n",
       "      <td>15123000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>W</td>\n",
       "      <td>7919</td>\n",
       "      <td>194.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3327470</td>\n",
       "      <td>0</td>\n",
       "      <td>8378575</td>\n",
       "      <td>73.773</td>\n",
       "      <td>C</td>\n",
       "      <td>12778</td>\n",
       "      <td>500.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>224.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3118781</td>\n",
       "      <td>0</td>\n",
       "      <td>2607840</td>\n",
       "      <td>400.000</td>\n",
       "      <td>R</td>\n",
       "      <td>12316</td>\n",
       "      <td>548.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mobile safari generic</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1136x640</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>mobile</td>\n",
       "      <td>iOS Device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3459772</td>\n",
       "      <td>0</td>\n",
       "      <td>12226544</td>\n",
       "      <td>31.950</td>\n",
       "      <td>W</td>\n",
       "      <td>9002</td>\n",
       "      <td>453.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2994681        0         242834          25.000         H   9803   \n",
       "1        3557242        0       15123000         117.000         W   7919   \n",
       "2        3327470        0        8378575          73.773         C  12778   \n",
       "3        3118781        0        2607840         400.000         R  12316   \n",
       "4        3459772        0       12226544          31.950         W   9002   \n",
       "\n",
       "   card2  card3       card4  card5  ...                  id_31  id_32  \\\n",
       "0  583.0  150.0        visa  226.0  ...           firefox 56.0   24.0   \n",
       "1  194.0  150.0  mastercard  166.0  ...                    NaN    NaN   \n",
       "2  500.0  185.0  mastercard  224.0  ...                    NaN    NaN   \n",
       "3  548.0  150.0        visa  195.0  ...  mobile safari generic   32.0   \n",
       "4  453.0  150.0        visa  226.0  ...                    NaN    NaN   \n",
       "\n",
       "       id_33           id_34  id_35 id_36 id_37  id_38  DeviceType  DeviceInfo  \n",
       "0  1920x1080  match_status:2      T     F     T      T     desktop     rv:56.0  \n",
       "1        NaN             NaN    NaN   NaN   NaN    NaN         NaN         NaN  \n",
       "2        NaN             NaN    NaN   NaN   NaN    NaN         NaN         NaN  \n",
       "3   1136x640  match_status:2      T     F     T      F      mobile  iOS Device  \n",
       "4        NaN             NaN    NaN   NaN   NaN    NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "jwNQSckh83QZ",
    "outputId": "0684051e-e2c0-4f33-80ab-18772b737189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59054 entries, 0 to 59053\n",
      "Columns: 434 entries, TransactionID to DeviceInfo\n",
      "dtypes: float64(385), int64(18), object(31)\n",
      "memory usage: 195.5+ MB\n"
     ]
    }
   ],
   "source": [
    "fraud_data.info()      # Returns a concise summary of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FjL68CPABH7"
   },
   "source": [
    "There are 434 columns with 59054 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "GcnguA4n9IsZ",
    "outputId": "9a0eeab4-ce2d-46d1-a522-0fbc6f18bfcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    57049\n",
       "1     2005\n",
       "Name: isFraud, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at the target variable\n",
    "fraud_data.isFraud.value_counts()       # The value_counts() function is used to get a Series containing counts of unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1L-PiTGd9d8b"
   },
   "source": [
    "We can notice, of 57049 observations / records only 2005 were fraud transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "92IJJSUf9cgq",
    "outputId": "83bc8f6f-5b07-44f6-c17a-8348d678c1bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    96.604802\n",
       "1     3.395198\n",
       "Name: isFraud, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data.isFraud.value_counts() / len(fraud_data) * 100       # Gets the percentage of unique values in the variable 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "sOl5jgYE9wLg",
    "outputId": "25ce0046-894a-436a-caaa-26a3a853f630"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x254a1eb7048>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can also use countplot form seaborn to plot the above information graphically.\n",
    "sns.countplot(fraud_data.isFraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBTv2idc-l1a"
   },
   "source": [
    "There are only 3% of the data which are fraud and the rest 97% are not fraud. This is clearly a class imbalance problem. In this notebook we will look to solve this type of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zks_JnKq_6tN"
   },
   "source": [
    "### Missing values\n",
    "Generally datasets always have some missing values. May be done during data collection, or due to some data validation rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "EHgTEqHR-Kk2",
    "outputId": "dec9d6e1-e41c-4e6b-da75-5a235f2d1bd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID      0.000000\n",
       "isFraud            0.000000\n",
       "TransactionDT      0.000000\n",
       "TransactionAmt     0.000000\n",
       "ProductCD          0.000000\n",
       "                    ...    \n",
       "id_36             75.945745\n",
       "id_37             75.945745\n",
       "id_38             75.945745\n",
       "DeviceType        75.979612\n",
       "DeviceInfo        79.813391\n",
       "Length: 434, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data.isnull().sum() / len(fraud_data) * 100   # To get percentage of missing data in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "02bqVJEbBxQc"
   },
   "source": [
    "Out of 434 columns, 414 have some missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjdy3vG5BmER"
   },
   "source": [
    "### Dealing with Missing Values\n",
    "*  Filling the missing values with right technique can change our results drastically. \n",
    "*  Also, there is no fixed rule of filling the missing values.\n",
    "*  No method is perfect for filling the missing values. We need to use our common sense, our logic, or may need to see what works for that particular data set.\n",
    "\n",
    "### Ways of dealing with missing values:\n",
    "\n",
    "**Default value:** One can fill the missing value by default value on the basis of one's 1) understanding of variable, 2) context / data insight or 3) common sense / logic. \n",
    "\n",
    "**Deleting:** Suppose in our dataset we have too many missing values in\n",
    "\n",
    "*  Column, we can drop the column\n",
    "*  Row, drop the row. Usually we do this for a large enough dataset.\n",
    "\n",
    "**Mean/Median/Mode - Imputation:** We fill missing values by mean or median or mode(i.e. maximum occuring value). Generally we use mean but if there are some outliers, we fill missing values with median. Mode is used to fill missing values for categorical column.\n",
    "\n",
    "[Data Cleaning in Python: the Ultimate Guide](https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZzsrECfAaSV"
   },
   "source": [
    "Eliminate columns with more than 20% missing values. Again this is very subjective and solely depends on the nature of the dataset and underlying context. We cannot generalize this procedure to all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qv5kehT_ALOO"
   },
   "outputs": [],
   "source": [
    "fraud_data = fraud_data[fraud_data.columns[fraud_data.isnull().mean() < 0.2]]    # Will keep those columns which has missing values less than 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysp1sghyDU1s"
   },
   "source": [
    "Here we will fill missing values of numerical variables (or columns) with mean value.\n",
    "\n",
    "**Variables are nothing but the column names. From here on we will start using the variables instead of column or column names.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSvRk1xJBAyR"
   },
   "outputs": [],
   "source": [
    "# filling missing values of numerical columns with mean value.\n",
    "num_cols = fraud_data.select_dtypes(include=np.number).columns      # getting all the numerical columns\n",
    "\n",
    "fraud_data[num_cols] = fraud_data[num_cols].fillna(fraud_data[num_cols].mean())   # fills the missing values with mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yK7ANXA5Eoeh"
   },
   "source": [
    "Filling missing values of categorical variables with mode. \n",
    "For those unattended, Mode is maximum occuring element in a variable.\n",
    "\n",
    "Why mode for categorical variables? - Let's take an example of categorical variable: \"Social Status\" with values: (Poor, Mid-income level, Rich). Can we find mean for social satus? No! So it may not be meaningful to go with mean in this case as it is not valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UeUz0v1ZEhAY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExplaining above line: (This explanation was given by Julilan Miranda in a group)\\n\\nThe above line of code is replacing the missing values in the columns in cat_cols with the mode (most repeated elements) of the non-missing values \\nin the same columns.\\nThe .iloc[0] attribute is selecting just the first mode returned, in case they are multiple values with the same highest frequency of occurrence. \\nPlease review the documentation for further clarifications on this regard: \\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mode.html\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = fraud_data.select_dtypes(include = 'object').columns    # getting all the categorical columns\n",
    "\n",
    "fraud_data[cat_cols] = fraud_data[cat_cols].fillna(fraud_data[cat_cols].mode().iloc[0])  # fills the missing values with maximum occuring element in the column\n",
    "\n",
    "\"\"\"\n",
    "Explaining above line: (This explanation was given by Julilan Miranda in a group)\n",
    "\n",
    "The above line of code is replacing the missing values in the columns in cat_cols with the mode (most repeated elements) of the non-missing values \n",
    "in the same columns.\n",
    "The .iloc[0] attribute is selecting just the first mode returned, in case they are multiple values with the same highest frequency of occurrence. \n",
    "Please review the documentation for further clarifications on this regard: \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mode.html\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ajSX2MhlFbWz",
    "outputId": "6ad39f08-d906-477d-873a-84befe2315b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID     0.0\n",
       "isFraud           0.0\n",
       "TransactionDT     0.0\n",
       "TransactionAmt    0.0\n",
       "ProductCD         0.0\n",
       "                 ... \n",
       "V317              0.0\n",
       "V318              0.0\n",
       "V319              0.0\n",
       "V320              0.0\n",
       "V321              0.0\n",
       "Length: 182, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look if there still exist any missing values\n",
    "fraud_data.isnull().sum() / len(fraud_data) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LW9XOIz6CGBj"
   },
   "source": [
    "Notice, now we don't have any column with missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKO21Qg8NSQc"
   },
   "source": [
    "### One Hot Encoding (Creating dummies for categorical columns)\n",
    "In this strategy, each category value is converted into a new column and assigned a 1 or 0 (notation for true/false) value to the column. In Python there is a class 'OneHotEncoder' in 'sklearn.preprocessing' to do this task, but here we will use pandas function 'get_dummies()'. This get_dummies() does the same work as done by 'OneHotEncoder' form sklearn.preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dq1OK7dWlFcx"
   },
   "source": [
    "![alt text](https://dphi.tech/wp-content/uploads/2020/06/ohe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvLkunI1pk2p"
   },
   "source": [
    "[Why One-Hot Encode Data in Machine Learning?](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydD6ZRZrMFFu"
   },
   "outputs": [],
   "source": [
    "fraud_data = pd.get_dummies(fraud_data, columns=cat_cols)    # earlier we have collected all the categorical columns in cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "id": "ulgnsjelE3I6",
    "outputId": "12a719e7-7225-490d-fc13-5fdf9b3867fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>...</th>\n",
       "      <th>P_emaildomain_web.de</th>\n",
       "      <th>P_emaildomain_windstream.net</th>\n",
       "      <th>P_emaildomain_yahoo.co.jp</th>\n",
       "      <th>P_emaildomain_yahoo.co.uk</th>\n",
       "      <th>P_emaildomain_yahoo.com</th>\n",
       "      <th>P_emaildomain_yahoo.com.mx</th>\n",
       "      <th>P_emaildomain_yahoo.de</th>\n",
       "      <th>P_emaildomain_yahoo.es</th>\n",
       "      <th>P_emaildomain_yahoo.fr</th>\n",
       "      <th>P_emaildomain_ymail.com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2994681</td>\n",
       "      <td>0</td>\n",
       "      <td>242834</td>\n",
       "      <td>25.000</td>\n",
       "      <td>9803</td>\n",
       "      <td>583.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3557242</td>\n",
       "      <td>0</td>\n",
       "      <td>15123000</td>\n",
       "      <td>117.000</td>\n",
       "      <td>7919</td>\n",
       "      <td>194.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3327470</td>\n",
       "      <td>0</td>\n",
       "      <td>8378575</td>\n",
       "      <td>73.773</td>\n",
       "      <td>12778</td>\n",
       "      <td>500.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3118781</td>\n",
       "      <td>0</td>\n",
       "      <td>2607840</td>\n",
       "      <td>400.000</td>\n",
       "      <td>12316</td>\n",
       "      <td>548.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3459772</td>\n",
       "      <td>0</td>\n",
       "      <td>12226544</td>\n",
       "      <td>31.950</td>\n",
       "      <td>9002</td>\n",
       "      <td>453.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt  card1  card2  card3  \\\n",
       "0        2994681        0         242834          25.000   9803  583.0  150.0   \n",
       "1        3557242        0       15123000         117.000   7919  194.0  150.0   \n",
       "2        3327470        0        8378575          73.773  12778  500.0  185.0   \n",
       "3        3118781        0        2607840         400.000  12316  548.0  150.0   \n",
       "4        3459772        0       12226544          31.950   9002  453.0  150.0   \n",
       "\n",
       "   card5  addr1  addr2  ...  P_emaildomain_web.de  \\\n",
       "0  226.0  269.0   87.0  ...                     0   \n",
       "1  166.0  181.0   87.0  ...                     0   \n",
       "2  224.0  284.0   60.0  ...                     0   \n",
       "3  195.0  441.0   87.0  ...                     0   \n",
       "4  226.0  264.0   87.0  ...                     0   \n",
       "\n",
       "   P_emaildomain_windstream.net  P_emaildomain_yahoo.co.jp  \\\n",
       "0                             0                          0   \n",
       "1                             0                          0   \n",
       "2                             0                          0   \n",
       "3                             0                          0   \n",
       "4                             0                          0   \n",
       "\n",
       "   P_emaildomain_yahoo.co.uk  P_emaildomain_yahoo.com  \\\n",
       "0                          0                        1   \n",
       "1                          0                        0   \n",
       "2                          0                        0   \n",
       "3                          0                        0   \n",
       "4                          0                        1   \n",
       "\n",
       "   P_emaildomain_yahoo.com.mx  P_emaildomain_yahoo.de  P_emaildomain_yahoo.es  \\\n",
       "0                           0                       0                       0   \n",
       "1                           0                       0                       0   \n",
       "2                           0                       0                       0   \n",
       "3                           0                       0                       0   \n",
       "4                           0                       0                       0   \n",
       "\n",
       "   P_emaildomain_yahoo.fr  P_emaildomain_ymail.com  \n",
       "0                       0                        0  \n",
       "1                       0                        0  \n",
       "2                       0                        0  \n",
       "3                       0                        0  \n",
       "4                       0                        0  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAkHAuGaE_Xo"
   },
   "source": [
    "If you notice, a lot of dummy variables are created like; **P_emaildomain_hotmail.com, P_emaildomain_hotmail.de,** etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQPCxfdDn0d5"
   },
   "source": [
    "#### Separate Input Features and Output Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQWaD295Lr7p"
   },
   "outputs": [],
   "source": [
    "# Separate input features and output feature\n",
    "X = fraud_data.drop(columns = ['isFraud'])       # input features\n",
    "Y = fraud_data.isFraud      # output feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwytZeEJGyzI"
   },
   "source": [
    "### Standardization / Normalization\n",
    "Often variables in a real dataset come with a wide range of data values.\n",
    "\n",
    "For example if you look at this fraud dataset, the variable 'TransactionAmt' has values in range 0.292 to 5279.95, while if you look at variable 'V14', it has values in the range 0.0 to 1.0. Basically, they are not on a common scale.\n",
    "\n",
    "**Now how does standardization/normalization help?**\n",
    "\n",
    "Performing standardization/normalization would bring all the variables in a dataset to a common scale so that it could further help in implementing various machine learning models (where standardization/normalization is a pre-requisite to apply such models). Again, don’t take this for granted, there are some smart algorithms which doesn’t need this and will explore one of them here.\n",
    "\n",
    "Well, we have also discussed about standardization / normalization in **Week#2 - Day#1** study material.\n",
    "\n",
    "Here we will use 'StandardScaler' class of 'sklearn.preprocessing' to scale our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UCqvAUtsn62V"
   },
   "source": [
    "![alt text](https://dphi.tech/wp-content/uploads/2020/06/sn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEFGJfmobfb0"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_features = StandardScaler().fit_transform(X)\n",
    "scaled_features = pd.DataFrame(data=scaled_features)\n",
    "scaled_features.columns= X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "colab_type": "code",
    "id": "_dqiqVCgojsS",
    "outputId": "da8d95dd-0c7a-419e-be48-eeb1ffc6a1ce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>C1</th>\n",
       "      <th>...</th>\n",
       "      <th>P_emaildomain_web.de</th>\n",
       "      <th>P_emaildomain_windstream.net</th>\n",
       "      <th>P_emaildomain_yahoo.co.jp</th>\n",
       "      <th>P_emaildomain_yahoo.co.uk</th>\n",
       "      <th>P_emaildomain_yahoo.com</th>\n",
       "      <th>P_emaildomain_yahoo.com.mx</th>\n",
       "      <th>P_emaildomain_yahoo.de</th>\n",
       "      <th>P_emaildomain_yahoo.es</th>\n",
       "      <th>P_emaildomain_yahoo.fr</th>\n",
       "      <th>P_emaildomain_ymail.com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.688548</td>\n",
       "      <td>-1.544958</td>\n",
       "      <td>-0.468203</td>\n",
       "      <td>-0.021940</td>\n",
       "      <td>1.412632</td>\n",
       "      <td>-0.286861</td>\n",
       "      <td>0.653753</td>\n",
       "      <td>-0.225982</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>-0.099186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021387</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>-0.009202</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>2.216281</td>\n",
       "      <td>-0.053413</td>\n",
       "      <td>-0.013649</td>\n",
       "      <td>-0.014839</td>\n",
       "      <td>-0.015399</td>\n",
       "      <td>-0.06253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.615662</td>\n",
       "      <td>1.681426</td>\n",
       "      <td>-0.073540</td>\n",
       "      <td>-0.406928</td>\n",
       "      <td>-1.078794</td>\n",
       "      <td>-0.286861</td>\n",
       "      <td>-0.804662</td>\n",
       "      <td>-1.144356</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>-0.099186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021387</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>-0.009202</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>-0.451206</td>\n",
       "      <td>-0.053413</td>\n",
       "      <td>-0.013649</td>\n",
       "      <td>-0.014839</td>\n",
       "      <td>-0.015399</td>\n",
       "      <td>-0.06253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.266093</td>\n",
       "      <td>0.219070</td>\n",
       "      <td>-0.258976</td>\n",
       "      <td>0.585989</td>\n",
       "      <td>0.881042</td>\n",
       "      <td>2.788641</td>\n",
       "      <td>0.605139</td>\n",
       "      <td>-0.069441</td>\n",
       "      <td>-10.788933</td>\n",
       "      <td>-0.099186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021387</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>-0.009202</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>-0.451206</td>\n",
       "      <td>-0.053413</td>\n",
       "      <td>-0.013649</td>\n",
       "      <td>-0.014839</td>\n",
       "      <td>-0.015399</td>\n",
       "      <td>-0.06253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.959645</td>\n",
       "      <td>-1.032167</td>\n",
       "      <td>1.140478</td>\n",
       "      <td>0.491581</td>\n",
       "      <td>1.188468</td>\n",
       "      <td>-0.286861</td>\n",
       "      <td>-0.099761</td>\n",
       "      <td>1.569022</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>-0.099186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021387</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>-0.009202</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>-0.451206</td>\n",
       "      <td>-0.053413</td>\n",
       "      <td>-0.013649</td>\n",
       "      <td>-0.014839</td>\n",
       "      <td>-0.015399</td>\n",
       "      <td>-0.06253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.043171</td>\n",
       "      <td>1.053404</td>\n",
       "      <td>-0.438389</td>\n",
       "      <td>-0.185621</td>\n",
       "      <td>0.580022</td>\n",
       "      <td>-0.286861</td>\n",
       "      <td>0.653753</td>\n",
       "      <td>-0.278162</td>\n",
       "      <td>0.077832</td>\n",
       "      <td>-0.082944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021387</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>-0.009202</td>\n",
       "      <td>-0.004115</td>\n",
       "      <td>2.216281</td>\n",
       "      <td>-0.053413</td>\n",
       "      <td>-0.013649</td>\n",
       "      <td>-0.014839</td>\n",
       "      <td>-0.015399</td>\n",
       "      <td>-0.06253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 249 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  TransactionDT  TransactionAmt     card1     card2     card3  \\\n",
       "0      -1.688548      -1.544958       -0.468203 -0.021940  1.412632 -0.286861   \n",
       "1       1.615662       1.681426       -0.073540 -0.406928 -1.078794 -0.286861   \n",
       "2       0.266093       0.219070       -0.258976  0.585989  0.881042  2.788641   \n",
       "3      -0.959645      -1.032167        1.140478  0.491581  1.188468 -0.286861   \n",
       "4       1.043171       1.053404       -0.438389 -0.185621  0.580022 -0.286861   \n",
       "\n",
       "      card5     addr1      addr2        C1  ...  P_emaildomain_web.de  \\\n",
       "0  0.653753 -0.225982   0.077832 -0.099186  ...             -0.021387   \n",
       "1 -0.804662 -1.144356   0.077832 -0.099186  ...             -0.021387   \n",
       "2  0.605139 -0.069441 -10.788933 -0.099186  ...             -0.021387   \n",
       "3 -0.099761  1.569022   0.077832 -0.099186  ...             -0.021387   \n",
       "4  0.653753 -0.278162   0.077832 -0.082944  ...             -0.021387   \n",
       "\n",
       "   P_emaildomain_windstream.net  P_emaildomain_yahoo.co.jp  \\\n",
       "0                     -0.022918                  -0.009202   \n",
       "1                     -0.022918                  -0.009202   \n",
       "2                     -0.022918                  -0.009202   \n",
       "3                     -0.022918                  -0.009202   \n",
       "4                     -0.022918                  -0.009202   \n",
       "\n",
       "   P_emaildomain_yahoo.co.uk  P_emaildomain_yahoo.com  \\\n",
       "0                  -0.004115                 2.216281   \n",
       "1                  -0.004115                -0.451206   \n",
       "2                  -0.004115                -0.451206   \n",
       "3                  -0.004115                -0.451206   \n",
       "4                  -0.004115                 2.216281   \n",
       "\n",
       "   P_emaildomain_yahoo.com.mx  P_emaildomain_yahoo.de  P_emaildomain_yahoo.es  \\\n",
       "0                   -0.053413               -0.013649               -0.014839   \n",
       "1                   -0.053413               -0.013649               -0.014839   \n",
       "2                   -0.053413               -0.013649               -0.014839   \n",
       "3                   -0.053413               -0.013649               -0.014839   \n",
       "4                   -0.053413               -0.013649               -0.014839   \n",
       "\n",
       "   P_emaildomain_yahoo.fr  P_emaildomain_ymail.com  \n",
       "0               -0.015399                 -0.06253  \n",
       "1               -0.015399                 -0.06253  \n",
       "2               -0.015399                 -0.06253  \n",
       "3               -0.015399                 -0.06253  \n",
       "4               -0.015399                 -0.06253  \n",
       "\n",
       "[5 rows x 249 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see how the data looks after scaling\n",
    "scaled_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ihcN9C3ELFat"
   },
   "source": [
    "### Splitting the dataset into train and test data\n",
    "**Why split the data into test sets and trainig sets?**\n",
    "\n",
    "Purpose of splitting data into the different category is to avoid overfitting.\n",
    "Please go through this awesome explanation, it will take only two minutes to go through it.  \n",
    "\n",
    "[In machine learning, what’s the purpose of splitting data up into test sets and training sets?](https://www.quora.com/In-machine-learning-what-s-the-purpose-of-splitting-data-up-into-test-sets-and-training-sets)\n",
    "\n",
    "**What is Train/Test split?**\n",
    "The data we use is split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model’s prediction on this subset.\n",
    "\n",
    "![alt text](https://lh3.googleusercontent.com/7T5aRDAlcsyt1p4R9qMrSznpgrEu5g2q8PkS-Jnh96fxfXP-ld3G-zCt1LfnpK3sOEdkbqLXyzjlQIf1VJFcxthhz12_-7Et7cxwFH6C)\n",
    "\n",
    "**Further Reading:** [Train/Test Split and Cross Validation in Python](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6#:~:text=Train%2FTest%20Split,to%20other%20data%20later%20on.&text=Pandas%20%E2%80%94%20to%20load%20the%20data,frame%20and%20analyze%20the%20data.)\n",
    "\n",
    "\n",
    "\n",
    "We will keep 30% of the data for test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9orHGVW2FwhR"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# X_train: independent feature data for training the model\n",
    "# Y_train: dependent feature data for training the model\n",
    "# X_test: independent feature data for testing the model; will be used to predict the target values\n",
    "# Y_test: original target values of X_test; We will compare this values with our predicted values.\n",
    " \n",
    "# test_size = 0.3: 30% of the data will go for test set and 70% of the data will go for train set\n",
    "# random_state = 42: this will fix the split i.e. there will be same split for each time you run the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0myjC1VxpHyx"
   },
   "source": [
    "# Dealing with Imbalanced Data\n",
    "Most machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce error. Again this can't be generalized and we must be very case specific depending on the nature of data and its underlying context.\n",
    "\n",
    "[Dealing with Imbalanced Data](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n",
    "\n",
    "## Different Techniques\n",
    "1. **Resampling Techniques - Oversample Minority Class:** Oversampling can be defined as adding more copies of the minority class. In other words, we are creating artificial/synthetic data of the minority class (or group). Oversampling could be a good choice when you don’t have a lot of data to work with.\n",
    "\n",
    "We will use the resampling module from Scikit-Learn library to randomly create artificial samples of data from the minority class.\n",
    "\n",
    "**Important Note**\n",
    "\n",
    "BEFORE you try any oversampling techniques **you must** split your data into train and test datasets! \n",
    "\n",
    "Why it should be done that? Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets. This can allow our model to simply memorize specific data points and cause overfitting and poor generalization to the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GD5K682rnmo"
   },
   "outputs": [],
   "source": [
    "# 'resample' is located under sklearn.utils\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUbIyU3psRZY"
   },
   "outputs": [],
   "source": [
    "# concatenate training data back together\n",
    "train_data = pd.concat([X_train, Y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqbcq46ssU1d"
   },
   "outputs": [],
   "source": [
    "# separate minority and majority class\n",
    "not_fraud = train_data[train_data.isFraud==0]\n",
    "fraud = train_data[train_data.isFraud==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phw2VsyTpGsZ"
   },
   "outputs": [],
   "source": [
    "# Unsample minority; we are oversampling the minority class to match the number of majority classs\n",
    "fraud_upsampled = resample(fraud,\n",
    "                           replace = True, # Sample with replacement\n",
    "                           n_samples = len(not_fraud), # Match number in majority class\n",
    "                           random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGeutaLase-O"
   },
   "outputs": [],
   "source": [
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_fraud, fraud_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "N15B1LCKs-pm",
    "outputId": "f317be10-e98a-4e44-aef6-aaebf4219e26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    39942\n",
       "0    39942\n",
       "Name: isFraud, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's check the classes count\n",
    "upsampled.isFraud.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JGaghffhtaO-"
   },
   "source": [
    "We can notice here after resampling we have an equal ratio of data points for each class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ybIjKt0trra"
   },
   "source": [
    "2. **Resampling Techniques - Undersample Majority Class:** Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.\n",
    "\n",
    "We will again use the resampling module from Scikit-Learn to randomly remove samples from the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmnu-yEftWeb"
   },
   "outputs": [],
   "source": [
    "# we are still using our separated class i.e. fraud and not_fraud from above\n",
    "# Again we are removing the observations of the majority class to mathch the number of minority class\n",
    "# downsample majority\n",
    "not_fraud_downsampled = resample(not_fraud,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(fraud), # match minority n\n",
    "                                random_state = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOVSn71bujZI"
   },
   "outputs": [],
   "source": [
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, fraud])    # Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "f1w8CVgjuoY4",
    "outputId": "315b5302-2ed6-4196-a581-3f302b8b992a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1395\n",
       "0    1395\n",
       "Name: isFraud, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the classes counts\n",
    "downsampled.isFraud.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx5AkNhxu0FD"
   },
   "source": [
    "Again, we have an equal ratio of fraud to not fraud data points, but in this case a much smaller quantity of data to train the model on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9vPjbAMvDFX"
   },
   "source": [
    "3. **Generate Synthetic Samples:** Here we will use imblearn’s SMOTE or Synthetic Minority Oversampling Technique. SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model.\n",
    "\n",
    "Again, it’s important to generate the new samples only in the training set to ensure our model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWkp26p-uwF2"
   },
   "outputs": [],
   "source": [
    "# import SMOTE \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state = 25)   # again we are eqalizing both the classes  @@ removed argument = ratio = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yf7sHze_wFJp"
   },
   "outputs": [],
   "source": [
    "# fit the sampling\n",
    "X_train, Y_train = sm.fit_sample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "15XOWmqPwULH",
    "outputId": "cef3b891-83c7-46c5-a012-1ef843c2e8d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([39942, 39942], dtype=int64))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train, return_counts=True)     # Y_train is numpy array, so unique() functions returns the count of all the unique elements in the array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8m9UNkRw1LV"
   },
   "source": [
    "The count of both the classes are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sl6WJORKxB-U"
   },
   "source": [
    "## Conclusion\n",
    "That's it for this notebook. We learned handling missing values, one hot encoding, standardization / normalization, what is imbalanced class and three techniques to deal with imbalanced classes.\n",
    "\n",
    "**Note:** I have not covered the part of model building here but soon we will have a session on '**Building your first Machine Learning Model**' by **Chanukya Patnaik**.\n",
    "\n",
    "# Thanks for reading the notebook! Hope you enjoyed it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTylM0lIybDT"
   },
   "source": [
    "References:\n",
    "1. [Dealing with Imbalanced Data by Tara Boyle](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n",
    "2. [Data Pre-processing - Handling missing values and dealing with class imbalance by Bharat Ram Ammu](https://www.youtube.com/watch?v=vksQx1JNo8Y)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Intro_to_Imbalanced_class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
